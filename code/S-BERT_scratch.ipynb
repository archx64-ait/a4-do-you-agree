{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "\n",
    "def get_memory_free_MiB(gpu_index):\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(int(gpu_index))\n",
    "    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    return mem_info.free // 1024 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "available memory of GPU 0: 11000 MiB \n",
      "\n",
      "GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
      "available memory of GPU 1: 738 MiB \n",
      "\n",
      "GPU 2: NVIDIA GeForce RTX 2080 Ti\n",
      "available memory of GPU 2: 4158 MiB \n",
      "\n",
      "GPU 3: NVIDIA GeForce RTX 2080 Ti\n",
      "available memory of GPU 3: 3598 MiB \n",
      "\n",
      "GPU 0 has the largest available VRAM: 11000 MiB\n"
     ]
    }
   ],
   "source": [
    "# I write this myself to select the GPU with highest available VRAM on puffer\n",
    "\n",
    "total_gpus = torch.cuda.device_count()\n",
    "largest_vram = 0\n",
    "gpu_index = 0\n",
    "\n",
    "for i in range(total_gpus):\n",
    "    new_vram = get_memory_free_MiB(i)\n",
    "    if new_vram > largest_vram:\n",
    "        largest_vram = new_vram\n",
    "        gpu_index = i\n",
    "    print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "    print(f'available memory of GPU {i}: {new_vram} MiB \\n')\n",
    "\n",
    "print(f'GPU {gpu_index} has the largest available VRAM: {largest_vram} MiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cuda device is set to: 0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(gpu_index)\n",
    "print(f'current cuda device is set to: {torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(3, 3, device=device)\n",
    "\n",
    "print(f\"Tensor is on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 69\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "mnli = datasets.load_dataset('glue', 'mnli')\n",
    "mnli_matched = mnli.filter(lambda x: x['label'] >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 392702\n",
      "    })\n",
      "    validation_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9815\n",
      "    })\n",
      "    validation_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9832\n",
      "    })\n",
      "    test_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9796\n",
      "    })\n",
      "    test_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(mnli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None), 'idx': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(mnli['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_range = 1000\n",
    "validation_range = 1000\n",
    "test_range = 100\n",
    "\n",
    "# The test set in both mnli_matched and mnli_mismatched does not have gold labels.\n",
    "# In MNLI, the test set is unannotated, so label = -1 is used as a placeholder.\n",
    "\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': mnli['train'].shuffle(seed=SEED).select(list(range(train_range))),\n",
    "    'validation': mnli['validation_matched'].shuffle(seed=SEED).select(list(range(validation_range))),\n",
    "    'test': mnli['validation_mismatched'].shuffle(seed=SEED).select(list(range(test_range))),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels in Train Set: [0 1 2]\n",
      "Unique Labels in Validation Set: [0 1 2]\n",
      "Unique Labels in Test Set: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Labels in Train Set:\", np.unique(raw_dataset['train']['label']))\n",
    "print(\"Unique Labels in Validation Set:\", np.unique(raw_dataset['validation']['label']))\n",
    "print(\"Unique Labels in Test Set:\", np.unique(raw_dataset['test']['label']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('models/my_tokenizer.json', 'r') as f:\n",
    "    tokenizer = json.load(f)\n",
    "word2id = tokenizer['word2id']\n",
    "id2word = tokenizer['id2word']\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def custom_tokenizer(sentences, max_length, padding='max_length', truncation=True):\n",
    "    tokenized_outputs = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.lower().split()\n",
    "        token_ids = [word2id.get(token, word2id['[UNK]']) for token in tokens]\n",
    "        \n",
    "        #add [CLS] at the start and [SEP] at the end for BERT compatibility\n",
    "        token_ids = [word2id['[CLS]']] + token_ids + [word2id['[SEP]']]\n",
    "        \n",
    "        #truncate if longer than max_length\n",
    "        if truncation and len(token_ids) > max_length:\n",
    "            token_ids = token_ids[:max_length-1] + [word2id['[SEP]']]\n",
    "        \n",
    "        attention_mask = [1] * len(token_ids)\n",
    "        \n",
    "        #pad if shorter than max_length\n",
    "        if padding == 'max_length':\n",
    "            padding_length = max_length - len(token_ids)\n",
    "            token_ids += [word2id['[PAD]']] * padding_length\n",
    "            attention_mask += [0] * padding_length\n",
    "        \n",
    "        tokenized_outputs[\"input_ids\"].append(token_ids)\n",
    "        tokenized_outputs[\"attention_mask\"].append(attention_mask)\n",
    "    return tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_seq_length = 1000\n",
    "    padding = 'max_length'\n",
    "    \n",
    "    #tokenize premise\n",
    "    premise_result = custom_tokenizer(\n",
    "        examples['premise'], \n",
    "        max_length=max_seq_length, \n",
    "        padding=padding, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    #tokenize hypothesis\n",
    "    hypothesis_result = custom_tokenizer(\n",
    "        examples['hypothesis'], \n",
    "        max_length=max_seq_length, \n",
    "        padding=padding, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    #extract labels\n",
    "    labels = examples[\"label\"]\n",
    "    \n",
    "    return {\n",
    "        \"premise_input_ids\": premise_result[\"input_ids\"],\n",
    "        \"premise_attention_mask\": premise_result[\"attention_mask\"],\n",
    "        \"hypothesis_input_ids\": hypothesis_result[\"input_ids\"],\n",
    "        \"hypothesis_attention_mask\": hypothesis_result[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['premise', 'hypothesis', 'label'])\n",
    "tokenized_datasets.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# initialize the dataloaders\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# iterate through the training dataloader\n",
    "for batch in train_dataloader:\n",
    "    print(batch['premise_input_ids'].shape)           # premise input IDs\n",
    "    print(batch['premise_attention_mask'].shape)      # premise attention mask\n",
    "    print(batch['hypothesis_input_ids'].shape)        # hypothesis input IDs\n",
    "    print(batch['hypothesis_attention_mask'].shape)   # hypothesis attention mask\n",
    "    print(batch['labels'].shape)                      # labels for classification\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)  # positional embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment embedding\n",
    "        self.norm = nn.LayerNorm(d_model)  # layer normalization\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = (\n",
    "            torch.arange(seq_len, dtype=torch.long)\n",
    "            .to(self.device)\n",
    "            .unsqueeze(0)\n",
    "            .expand_as(x)\n",
    "        )  # create position indices\n",
    "        embedding = (\n",
    "            self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        )  # sum all embeddings\n",
    "        return self.norm(embedding)  # apply layer normalization\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        d_k,\n",
    "        n_segments,\n",
    "        vocab_size,\n",
    "        max_len,\n",
    "        device,\n",
    "    ):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding(\n",
    "            vocab_size, max_len, n_segments, d_model, device\n",
    "        )  # embedding layer\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)]\n",
    "        )  # transformer encoder layers\n",
    "        self.fc = nn.Linear(d_model, d_model)  # fully connected layer for hidden states\n",
    "        self.activ = nn.Tanh()  # activation function\n",
    "        self.linear = nn.Linear(d_model, d_model)  # another linear layer\n",
    "        self.norm = nn.LayerNorm(d_model)  # layer normalization\n",
    "        self.classifier = nn.Linear(d_model, 2)  # classifier head for predictions\n",
    "        self.decoder = nn.Linear(\n",
    "            d_model, vocab_size, bias=False\n",
    "        )  # decoder for language modeling\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(vocab_size))  # bias for decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        output = self.embedding(input_ids, segment_ids)  # get embeddings\n",
    "        enc_self_attn_mask = get_attn_pad_mask(\n",
    "            input_ids, input_ids, self.device\n",
    "        )  # attention mask\n",
    "        for layer in self.layers:\n",
    "            output, _ = layer(\n",
    "                output, enc_self_attn_mask\n",
    "            )  # pass through transformer layers\n",
    "        return output  # return hidden states\n",
    "\n",
    "    def get_last_hidden_state(self, input_ids):\n",
    "        segment_ids = torch.zeros_like(input_ids).to(\n",
    "            self.device\n",
    "        )  # default segment ids as zeros\n",
    "        output = self.embedding(input_ids, segment_ids)  # get embeddings\n",
    "        enc_self_attn_mask = get_attn_pad_mask(\n",
    "            input_ids, input_ids, self.device\n",
    "        )  # attention mask\n",
    "        for layer in self.layers:\n",
    "            output, _ = layer(\n",
    "                output, enc_self_attn_mask\n",
    "            )  # pass through transformer layers\n",
    "        return output  # return last hidden state\n",
    "\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, device):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).to(device)  # mask padding tokens\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # expand mask to all heads\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(\n",
    "            n_heads, d_model, d_k, device\n",
    "        )  # multi-head self-attention\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(\n",
    "            d_model, d_ff\n",
    "        )  # position-wise feed-forward network\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(\n",
    "            enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask\n",
    "        )  # self-attention mechanism\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)  # position-wise feed-forward\n",
    "        return enc_outputs, attn  # return outputs and attention weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k, device):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)  # query projection\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)  # key projection\n",
    "        self.W_V = nn.Linear(d_model, d_k * n_heads)  # value projection\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        residual, batch_size = Q, Q.size(0)  # residual connection\n",
    "        q_s = (\n",
    "            self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        )  # project queries\n",
    "        k_s = (\n",
    "            self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        )  # project keys\n",
    "        v_s = (\n",
    "            self.W_V(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        )  # project values\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(\n",
    "            1, self.n_heads, 1, 1\n",
    "        )  # repeat mask for all heads\n",
    "        context, attn = ScaledDotProductAttention(self.d_k, self.device)(\n",
    "            q_s, k_s, v_s, attn_mask\n",
    "        )  # scaled dot-product attention\n",
    "        context = (\n",
    "            context.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        )  # concatenate attention heads\n",
    "        output = nn.Linear(self.n_heads * self.d_k, self.d_model).to(self.device)(\n",
    "            context\n",
    "        )  # final linear layer\n",
    "        return (\n",
    "            nn.LayerNorm(self.d_model).to(self.device)(output + residual),\n",
    "            attn,\n",
    "        )  # add & normalize\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, device):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_k])).to(device)  # scaling factor\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / self.scale  # scaled dot-product\n",
    "        scores.masked_fill_(attn_mask, -1e9)  # apply attention mask\n",
    "        attn = nn.Softmax(dim=-1)(scores)  # softmax to get attention weights\n",
    "        context = torch.matmul(attn, V)  # weighted sum of values\n",
    "        return context, attn  # return context and attention weights\n",
    "\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)  # first feed-forward layer\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)  # second feed-forward layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(nn.functional.gelu(self.fc1(x)))  # apply gelu activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1000\n",
    "n_layers = 12\n",
    "n_heads = 12\n",
    "d_model = 768\n",
    "d_ff = d_model * 4\n",
    "d_k = d_v = 64\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): Embedding(\n",
       "    (tok_embed): Embedding(23069, 768)\n",
       "    (pos_embed): Embedding(1000, 768)\n",
       "    (seg_embed): Embedding(2, 768)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x EncoderLayer(\n",
       "      (enc_self_attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (pos_ffn): PoswiseFeedForwardNet(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (decoder): Linear(in_features=768, out_features=23069, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize and load BERT model\n",
    "model = BERT(\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    d_model=d_model,\n",
    "    d_ff=d_ff,\n",
    "    d_k=d_k,\n",
    "    n_segments=n_segments,\n",
    "    vocab_size=vocab_size,\n",
    "    max_len=max_len,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('models/bert_model_1.pth', map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "## Classification Objective Function \n",
    "We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference  $\\lvert u - v \\rvert $ and multiply the result with the trainable weight  $ W_t âˆˆ  \\mathbb{R}^{3n \\times k}  $:\n",
    "\n",
    "$ o = \\text{softmax}\\left(W^T \\cdot \\left(u, v, \\lvert u - v \\rvert\\right)\\right) $\n",
    "\n",
    "where $n$ is the dimension of the sentence embeddings and k the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n",
    "\n",
    "## Regression Objective Function. \n",
    "The cosine similarity between the two sentence embeddings $u$ and $v$ is computed (Figure 2). We use means quared-error loss as the objective function.\n",
    "\n",
    "(Manhatten / Euclidean distance, semantically  similar sentences can be found.)\n",
    "\n",
    "<img src=\"./figures/sbert-architecture.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurations(u,v):\n",
    "    # build the |u-v| tensor\n",
    "    uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
    "    uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "    \n",
    "    # concatenate u, v, |u-v|\n",
    "    x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "    return x\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/sbert-ablation.png\" width=\"350\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_head = torch.nn.Linear(768*3, 3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124974/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# and setup a warmup for the first ~10% steps\n",
    "total_steps = int(len(raw_dataset) / batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler.step()\n",
    "\n",
    "scheduler_classifier = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler_classifier.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise Max input_id: tensor(22619)\n",
      "Premise Min input_id: tensor(0)\n",
      "Hypothesis Max input_id: tensor(21349)\n",
      "Hypothesis Min input_id: tensor(0)\n",
      "Vocab size: 23069\n"
     ]
    }
   ],
   "source": [
    "premise_input_ids = batch['premise_input_ids']\n",
    "print(\"Premise Max input_id:\", torch.max(premise_input_ids))\n",
    "print(\"Premise Min input_id:\", torch.min(premise_input_ids))\n",
    "\n",
    "hypothesis_input_ids = batch['hypothesis_input_ids']\n",
    "print(\"Hypothesis Max input_id:\", torch.max(hypothesis_input_ids))\n",
    "print(\"Hypothesis Min input_id:\", torch.min(hypothesis_input_ids))\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8180934c9f04b3aa1bc2696982149fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss = 3.666999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f1ab0f12fe4ff0bb2f5203ad2f62c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | loss = 3.084264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a0a13d88cf40c996f9d00e820ce04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | loss = 3.534136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146ed265a1a34c8c9fba6ecdcaea18aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | loss = 2.575287\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epoch = 4  # 1 is enough, can increase if you want more epoches\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()  \n",
    "    classifier_head.train()\n",
    "\n",
    "    # training loop with tqdm for progress bar\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "        \n",
    "        # move tensors to GPU (or CPU) device\n",
    "        premise_input_ids = batch['premise_input_ids'].to(device)\n",
    "        hypothesis_input_ids = batch['hypothesis_input_ids'].to(device)\n",
    "        premise_attention_mask = batch['premise_attention_mask'].to(device)\n",
    "        hypothesis_attention_mask = batch['hypothesis_attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        premise_segment_ids = torch.zeros_like(premise_input_ids).to(device)\n",
    "        hypothesis_segment_ids = torch.ones_like(hypothesis_input_ids).to(device)\n",
    "        \n",
    "        u = model(premise_input_ids, premise_segment_ids)\n",
    "        v = model(hypothesis_input_ids, hypothesis_segment_ids)\n",
    "        \n",
    "        u_mean_pool = mean_pool(u, premise_attention_mask)\n",
    "        v_mean_pool = mean_pool(v, hypothesis_attention_mask)\n",
    "        \n",
    "        uv = torch.sub(u_mean_pool, v_mean_pool)\n",
    "        uv_abs = torch.abs(uv)\n",
    "        \n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1)\n",
    "        \n",
    "        logits = classifier_head(x)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        #backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_classifier.step()\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        scheduler_classifier.step()\n",
    "        \n",
    "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 0.9982\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "classifier_head.eval()\n",
    "total_similarity = 0.0\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        # move tensors to the GPU (or CPU) device\n",
    "        premise_input_ids = batch['premise_input_ids'].to(device)\n",
    "        hypothesis_input_ids = batch['hypothesis_input_ids'].to(device)\n",
    "        premise_attention_mask = batch['premise_attention_mask'].to(device)\n",
    "        hypothesis_attention_mask = batch['hypothesis_attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # create segment IDs for premise and hypothesis\n",
    "        premise_segment_ids = torch.zeros_like(premise_input_ids).to(device)\n",
    "        hypothesis_segment_ids = torch.ones_like(hypothesis_input_ids).to(device)\n",
    "        \n",
    "        # extract last hidden states using custom BERT for premise and hypothesis\n",
    "        u = model.get_last_hidden_state(premise_input_ids)\n",
    "        v = model.get_last_hidden_state(hypothesis_input_ids)\n",
    "        \n",
    "        # get the mean pooled vectors for premise and hypothesis\n",
    "        u_mean_pool = mean_pool(u, premise_attention_mask)  # [B, H]\n",
    "        v_mean_pool = mean_pool(v, hypothesis_attention_mask)  # [B, H]\n",
    "        \n",
    "        # compute cosine similarity for each sample in the batch\n",
    "        # regression objective function\n",
    "        cos_sim = (u_mean_pool * v_mean_pool).sum(dim=1) / (\n",
    "            torch.norm(u_mean_pool, dim=1) * torch.norm(v_mean_pool, dim=1) + 1e-8\n",
    "        )\n",
    "        # average the similarity over the batch (a scalar)\n",
    "        similarity_score = cos_sim.mean().item()\n",
    "        total_similarity += similarity_score\n",
    "        \n",
    "        # build the |u-v| tensor\n",
    "        uv = torch.sub(u_mean_pool, v_mean_pool)\n",
    "        uv_abs = torch.abs(uv)\n",
    "        \n",
    "        # concatenate u, v, |u-v|\n",
    "        # classfication objective function\n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1)\n",
    "        \n",
    "        # pass concatenated tensor through classifier head\n",
    "        logits = classifier_head(x)\n",
    "        \n",
    "        # get the predicted class (0, 1, or 2 for MNLI)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "    \n",
    "average_similarity = total_similarity / len(eval_dataloader)\n",
    "\n",
    "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine Similarity: 0.9950\n"
     ]
    }
   ],
   "source": [
    "def calculate_similarity(model, sentence_a, sentence_b, device, max_length=128):\n",
    "    # use the custom tokenizer to tokenize premise and hypothesis separately\n",
    "    inputs_a = custom_tokenizer([sentence_a], max_length=max_length, padding='max_length', truncation=True)\n",
    "    inputs_b = custom_tokenizer([sentence_b], max_length=max_length, padding='max_length', truncation=True)\n",
    "    \n",
    "    # convert lists to torch tensors and move to device\n",
    "    input_ids_a = torch.tensor(inputs_a['input_ids']).to(device)\n",
    "    attention_a = torch.tensor(inputs_a['attention_mask']).to(device)\n",
    "    input_ids_b = torch.tensor(inputs_b['input_ids']).to(device)\n",
    "    attention_b = torch.tensor(inputs_b['attention_mask']).to(device)\n",
    "    \n",
    "    # create segment IDs for premise and hypothesis\n",
    "    premise_segment_ids = torch.zeros_like(input_ids_a).to(device)\n",
    "    hypothesis_segment_ids = torch.ones_like(input_ids_b).to(device)\n",
    "    \n",
    "    # use the model's helper method to get token embeddings\n",
    "    u = model.get_last_hidden_state(input_ids_a)\n",
    "    v = model.get_last_hidden_state(input_ids_b)\n",
    "    \n",
    "    # get mean pooled sentence embeddings\n",
    "    u_mean = mean_pool(u, attention_a).detach().cpu().numpy().squeeze()\n",
    "    v_mean = mean_pool(v, attention_b).detach().cpu().numpy().squeeze()\n",
    "    \n",
    "    # calculate cosine similarity (result is a scalar)\n",
    "    similarity_score = np.dot(u_mean, v_mean) / (np.linalg.norm(u_mean) * np.linalg.norm(v_mean) + 1e-8)\n",
    "    return similarity_score\n",
    "\n",
    "# sentences for similarity calculation\n",
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
    "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
    "\n",
    "similarity = calculate_similarity(model, sentence_a, sentence_b, device)\n",
    "print(f\"cosine Similarity: {similarity:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'models/custom_bert_mnli.pth')\n",
    "torch.save(classifier_head.state_dict(), 'models/classifier_head.pth')\n",
    "\n",
    "with open('models/custom_tokenizer.json', 'w') as f:\n",
    "    json.dump(word2id, f)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
